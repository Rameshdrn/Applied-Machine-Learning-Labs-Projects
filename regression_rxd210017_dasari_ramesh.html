# -*- coding: utf-8 -*-
"""REGRESSION_RXD210017_DASARI_RAMESH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g_Y6czg8iAyY3YgUAe0k10OObFwHfhqJ

# Importing the packages
"""

!pip install xgboost
!pip install plotly

import warnings

warnings.filterwarnings("ignore")

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Library to split data
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder,StandardScaler

# libaries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 100)
# for cross validation and hyperparameter tuning to find optimal
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

# Libraries different ensemble classifiers
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier

from xgboost import XGBClassifier

# Libraries to get different metric scores
from sklearn import metrics
from sklearn.metrics import (
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
)

from sklearn import metrics 
import plotly.express as px
from sklearn.linear_model import LinearRegression# import linear regression models
from sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor# import random forest regressor
# importing the perofrmace metrics
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
# check xgboost version
from  xgboost import XGBRegressor

# loading the car price dataset
dataframe = pd.read_csv("Car_prices_project_2.csv")
dataframe

"""# Data Analysis"""

# some basic stats about the data
dataframe.describe()

# check data types
dataframe.info()

# check for the missing values
dataframe.isna().sum()

"""#### Observations:
- There are lots of missing 27k missing values in the feature genreration _name
"""

# Replace the missign values by NA 
dataframe = dataframe.fillna("NA")

# again check for the missing values
dataframe.isna().sum()

# seperate the class label from features
X= dataframe.drop("price",axis=1)
y= dataframe["price"]

numeric_cols = X.select_dtypes(['float64','int64']).columns
categoric_cols = X.select_dtypes('object').columns
X_numeric = X[numeric_cols]
X_categoric = X[categoric_cols]

# label encoding the categoric feature
le = LabelEncoder()
for i in X_categoric.columns:
    X_categoric[i]= le.fit_transform(X_categoric[i])

X_categoric

X = pd.concat([X_numeric,X_categoric], axis=1)

# scaling of numeric features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# split dataset for train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

"""# Machine Learning Models"""

def performance_metrics(model,X,y):
    print(model)
    y_pred = model.predict(X)
    print("MSE : ",mean_squared_error(y_pred,y))
    print("MAE :",mean_absolute_error(y_pred,y))
    print("R2_score :",r2_score(y_pred,y))
    print("Actual Values:", (y[:10]))
    print("Predicted Values:",(y_pred[:10]))
    fig=px.scatter(x=y,y=y_pred,title="Actual vs Predicted plot")
    fig.show()

xgb = XGBRegressor().fit(X_train, y_train)
rs = RandomForestRegressor(random_state=17, n_jobs=-1).fit(X_train, y_train)

print("For Training Data")
performance_metrics(xgb ,X_train, y_train)
print("For Testing Data")
performance_metrics(xgb,X_test, y_test)

print("For Training Data")
performance_metrics(rs,X_train, y_train)
print("For Testing Data")
performance_metrics(rs,X_test, y_test)

"""# HyperParameter Tuning"""

def grid_search (estimator, param):
    gscv = GridSearchCV(estimator, param, cv=10, n_jobs=-1, scoring='neg_mean_squared_error')
    gscv.fit(X_train, y_train)
    return gscv.best_estimator_

def random_search (estimator, param):
    rscv = RandomizedSearchCV(estimator, param, cv=10, n_jobs=-1, scoring='neg_mean_squared_error')
    rscv.fit(X_train, y_train)
    return rscv.best_estimator_

rs_params = {
    "max_depth": list(np.arange(5, 15, 5)),
    "max_features": ["sqrt", "log2"],
    "min_samples_split": [5, 7],
    "n_estimators": np.arange(100, 551, 50),
}

xgb_params = parameters = {
    "n_estimators": np.arange(150, 250, 50),
    "scale_pos_weight": [1, 2],
    "subsample": [0.9, 1],
    "learning_rate": np.arange(0.1, 0.21, 0.1),
    "gamma": [3, 5],
    "colsample_bytree": [0.8, 0.9],
    "colsample_bylevel": [0.9, 1],
}

rscv_param = [xgb_params, rs_params]

best_est = []
n=0
for model in [xgb, rs]:
    best_est.append(random_search(model, rscv_param[n]))
    n+=1

"""# Printing best estimators
best_est
"""

# Performance Evaluation
for model in best_est:
    print("For Training Data")
    performance_metrics(model,X_train, y_train)
    print("For Testing Data")
    performance_metrics(model,X_test, y_test)

"""Observations:
- XGB Regressor works  better as compared to the Random Forest Regressor.
"""

feature_names = dataframe.drop('price',axis=1).columns
importances = xgb.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12, 12))
plt.title("Feature Importances")
plt.barh(range(len(indices)), importances[indices], color="violet", align="center")
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel("Relative Importance")
plt.show()

